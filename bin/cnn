#!/usr/bin/env python2
# -*- coding: utf-8; mode: python -*-
################################################################################
# cnn: EnhanceR PROCESS UC1
################################################################################
# For copyright see the `LICENSE` file.
#
# This file is part of PROCESS_UC1.
################################################################################
# kludge, waiting for proper packaging...
import sys, argparse
sys.path += ['lib/python2.7/', '../lib/python2.7/']
from defaults import (
    HAS_SKIMAGE_VIEW,
    HAS_TENSORFLOW,
    CONFIG_FILE,
    def_config,
    time_stamp
)

import pprint as pp

# system deps
import matplotlib
# *must* stay here, before downstream imports of matplotlib.pyplot
matplotlib.use('agg')

import os, traceback, errno
from os import listdir
from os.path import join, isfile, exists, splitext
from random import shuffle
import cv2
import numpy as np
from PIL import Image
from skimage.transform.integral import integral_image, integrate
if HAS_SKIMAGE_VIEW:
    from skimage.viewer import ImageViewer
    # import skimage
    from skimage import io
if HAS_TENSORFLOW:
    from keras import backend as K
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
    import tensorflow as tf
    import tflearn
    from tflearn.data_utils import shuffle, to_categorical
    from tflearn.layers.core import input_data, dropout, fully_connected
    from tflearn.layers.conv import conv_2d, max_pool_2d
    from tflearn.layers.estimator import regression
    import horovod.keras as hvd

import h5py as hd
import shutil
import math
################################################################################
# package deps
from functions import (
    debug,
    logger,
    parseConfig,
    validate_non_neg_int,
    validate_pipeline,
    log_init,
)
from pipeline import extract, load, train
from extract_xml import *
from integral import patch_sampling_using_integral
if HAS_TENSORFLOW:
    from models import *
###############################################################################
'''
cnn -- EnahnceR PROCESS_UC1 launch script

Usage
=====

High resolution patch extraction
--------------------------------

    $ bin/cnn [OPTIONS] [STEP1 [STEP2...]]


Options
-------

Legend: <type>(<default value>)

    --gpu-id, -g N
        <int>(0). GPU ID to use; must be >= 0. Currently *unused*!

    --patients, -p PATIENT...
        <list>([]). A space-separated list of patient cases. It must be a subset
        of the whole datasets; each item must match the regex
        `config['camelyon17'] => 'patient_name_regex'`

    --seed, -s S
        <int>(). Random seed to initialize the Numpy lib

    --log-level, -l LEVEL
        <str>('info'). Logging level (notset, debug, info, warn, error, critical)

    --results-dir, -r  RESULTS-DIR
        <str>('results'). Path to the results dir. Overrides
        `config[settings][results_dir]`.

Pipeline
========

For now only two pipeline steps are allowed. When none are given, the program
works in patch extraction mode. Mode summary:

#  mode                           arg1     arg2
------------------------------------------------
1  patch extraction               extract  -
2  patch extraction + training    extract  train
3  training existing patches      load     train

**Developer notes.** Programmatic pipeline definitions are in `defaults.py`


TO-DO: what about adding command line shortcuts for pipelines?


Patch extraction
----------------

Just run:

    $ bin/cnn [OPTIONS]


Network training
----------------


Extract patches and train:

    $ bin/cnn [OPTIONS] train


Load patches and train:

    $ bin/cnn [OPTIONS] load train


Program output
==============

Results are stored in a directory whose root path is set (see L<Configuration>) by option

  settings => results_dir/


Configuration File
==================

The default configuration file is 'config.cfg'. It can be overridden by the
ENV variable 'PROCESS_UC1__CONFIG_FILE'. So. f.i., one would call the program:

    $ PROCESS_UC1__CONFIG_FILE=path/to/my/config.ini bin/cnn ...

See 'etc/config.ini' for a description of the available options (TO-DO). Mind
that some of them can be overridden on the command line, notably:

    gpu-id => GPU


To-Do
=====

...many things ;-)

'''
################################################################################
# init
################################################################################

# command line options override default and config file's ones
parser = argparse.ArgumentParser(description='EnhanceR PROCESS_UC1 master script.')
parser.add_argument(
    'pipeline',
    # metavar='STEP', # no metavar, else choices won't be proposed on -h
    type=str, # pipeline composition is validated downstream
    nargs='+',
    choices=('extract', 'load', 'train'),
    help='2-step pipeline'
)
parser.add_argument(
    '-c', '--config-file',
    dest='config_file',
    type=str,
    default=CONFIG_FILE,
    help='configuration file. Default: see `defaults.CONFIG_FILE`'
)
parser.add_argument(
    '-g', '--gpu-id',
    dest='gpu_id',
    type=validate_non_neg_int,
    # default=0, # default from config['GPU']
    help='GPU ID to use'
)
parser.add_argument(
    '-l', '--log-level',
    dest='log_level',
    # metavar='LEVEL', # no metavar, else choices won't be proposed on -h
    default='info',
    type=str, # validated downstream
    choices=('notset', 'debug', 'info', 'warn', 'error', 'critical'),
    help='logging level'
)
parser.add_argument(
    '-s', '--seed',
    dest='seed',
    type=int,
    default=0,
    help='Numpy random seed'
)
parser.add_argument(
    '-p', '--patients',
    dest='patients',
    nargs='+',
    type=str,
    help='Patient list. Each item must match `[camelyon17] => patient_name_regex` in config file'
)
parser.add_argument(
    '-r', '--results-dir',
    dest='results_dir',
    metavar='RESULTS-DIR',
    type=str,
    help='results dir name. Overrides `config[settings][results_dir]`. Default: "results/"'
)

# easier with a dict
args = vars(parser.parse_args())

# Config file is parsed _once_ for ever. All options are now stored in a dict
# (same structure as for `defaults::def_config`)
config = parseConfig(args['config_file'], def_config)

# settings override, None by args default
for k in ('gpu_id', 'results_dir'):
    if args[k]:
        config['settings'][k] = args[k]

new_results_dir = os.path.expanduser(config['settings']['results_dir'])
try:
    os.makedirs(new_results_dir)
except OSError as e:
    if e.errno == errno.EEXIST:
        pass
    else:
        sys.exit('{}: cannot make results dir: {}\n'.format(new_results_dir, e))

logger = log_init(
    log_level=args['log_level'],
    log_fname=os.path.join(new_results_dir, 'INFO.log')
)

shutil.copy2(src=args['config_file'], dst=new_results_dir)

logger.debug(
    '[cnn] saving configuration file ({}) to results dir: {}'.format(args['config_file'], new_results_dir)
)

logger.debug('[cnn] config:\n%s\n' % pp.pformat(config))
logger.debug('[cnn] args:\n%s\n' % pp.pformat(args))

try:
    validate_pipeline(args['pipeline'])
except argparse.ArgumentTypeError as e:
    sys.exit('[error] invalid pipeline definition: {}\n'.format(e))

logger.info('[cnn] Setting random seed: %s' % args['seed'])
np.random.seed(args['seed'])

################################################################################
# pipeline run
################################################################################

for step in args['pipeline']:
    logger.info('[cnn] running step: {}'.format(step))
    try:
        eval(step)(config, new_results_dir, logger=logger, patients=args['patients'])
    except Exception as e:
        logger.debug(traceback.format_exc())
        sys.exit('[cnn] {}: pipeline step failed: {} '.format(step, e))


sys.exit(0)
