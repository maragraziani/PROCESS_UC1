#!/usr/bin/env python2
# -*- coding: utf-8; mode: python -*-
################################################################################
# cnn: EnhanceR PROCESS UC1
################################################################################
# For copyright see the `LICENSE` file.
#
# This file is part of PROCESS_UC1.
################################################################################
# kludge, waiting for proper packaging...
import sys, argparse
sys.path += ['lib/python2.7/', '../lib/python2.7/']
from defaults import (
    HAS_SKIMAGE_VIEW,
    HAS_TENSORFLOW,
    CONFIG_FILE,
    def_config,
    time_stamp
)

import pprint as pp

# system deps
import matplotlib
# *must* stay here, before downstream imports of matplotlib.pyplot
matplotlib.use('agg')

import os, traceback, errno
from os import listdir
from os.path import join, isfile, exists, splitext
from random import shuffle
import cv2
import numpy as np
from PIL import Image
from skimage.transform.integral import integral_image, integrate
if HAS_SKIMAGE_VIEW:
    from skimage.viewer import ImageViewer
    # import skimage
    from skimage import io
if HAS_TENSORFLOW:
    from keras import backend as K
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
    import tensorflow as tf
    import tflearn
    from tflearn.data_utils import shuffle, to_categorical
    from tflearn.layers.core import input_data, dropout, fully_connected
    from tflearn.layers.conv import conv_2d, max_pool_2d
    from tflearn.layers.estimator import regression
    import horovod.keras as hvd

import h5py as hd
import shutil
import math
################################################################################
# package deps
from functions import (
    debug,
    logger,
    parseConfig,
    validate_non_neg_int,
    validate_pipeline,
    log_init,
    wlog
)
from pipeline import extract, load, train
from extract_xml import *
from integral import patch_sampling_using_integral
if HAS_TENSORFLOW:
    from models import *
###############################################################################
'''
cnn.py -- EnahnceR PROCESS_UC1 launch script

Usage
=====

High resolution patch extraction
--------------------------------

    $ bin/cnn.py [OPTIONS] [STEP1 [STEP2...]]


Options
-------

    --gpu-id, -g N
        (int>=0). Force GPU ID to use. Default '0'.

    --xml-ann-file, -x XML_ANNOTATION_FILE
        (str). Path to a single XML annotation file. If given a single WSI slide will
        be processed for any configured 'centre'. Default: None -- will scan data input dir.

    --seed, -s S
        (int). Random seed to initialize the Numpy lib

    --log-level, -l LEVEL
        (str). Logging level (notset, debug, info, warn, error, critical). Default: info.

    --results-sdir, -r  RESULTS-SUBDIR
        (str). Results subdir name under `config[settings][results_dir]`.
        Default: auto-generated as a "MMDD-hhmm" timestamp

Pipeline
========

For now only two pipeline steps are allowed. When none are given, the program
works in patch extraction mode. Mode summary:

#  mode                           arg1     arg2
------------------------------------------------
1  patch extraction               extract  -
2  patch extraction + training    extract  train
3  training existing patches      load     train

**Developer notes.** Programmatic pipeline definitions are in `defaults.py`


TO-DO: what about adding command line shortcuts for pipelines?


Patch extraction
----------------

Just run:

    $ bin/cnn.py [OPTIONS]


Network training
----------------


Extract patches and train:

    $ bin/cnn.py [OPTIONS] train


Load patches and train:

    $ bin/cnn.py [OPTIONS] load train


Program output
==============

Results are stored in a directory whose root path is set (see L<Configuration>) by option

  settings => results_dir/<sub-dir>

with <sub-dir> in the form of a time-stamp 'MMDD-hhmm' (MM for month, DD for day,
hh for hour and mm for minute), or set via CL option `--results-sdir.


Configuration File
==================

The default configuration file is 'config.cfg'. It can be overridden by the
ENV variable 'PROCESS_UC1__CONFIG_FILE'. So. f.i., one would call the program:

    $ PROCESS_UC1__CONFIG_FILE=path/to/my/config.ini bin/cnn ...

See 'etc/config.ini' for a description of the available options (TO-DO). Mind
that some of them can be overridden on the command line, notably:

    gpu-id => GPU

'''

# Config file is parsed _once_ for ever
# print '[cnn][config] Loading system configurations from: ', CONFIG_FILE
# All options are now stored in a dict (same structure as for `defaults::def_config`)
config = parseConfig(CONFIG_FILE, def_config)

# command line options override default and config file's ones
parser = argparse.ArgumentParser(description='EnhanceR PROCESS_UC1 master script.')
parser.add_argument(
    'pipeline',
    # metavar='STEP', # no metavar, else choices won't be proposed on -h
    type=str, # pipeline composition is validated downstream
    nargs='+',
    choices=('extract', 'load', 'train'),
    help='2-step pipeline'
)
parser.add_argument(
    '-g', '--gpu-id',
    dest='gpu_id',
    metavar='GPU-ID',
    type=validate_non_neg_int,
    # default=0, # default from config['GPU']
    help='GPU ID to use'
)
parser.add_argument(
    '-l', '--log-level',
    dest='log_level',
    # metavar='LEVEL', # no metavar, else choices won't be proposed on -h
    default='info',
    type=str, # validated downstream
    choices=('notset', 'debug', 'info', 'warn', 'error', 'critical'),
    help='logging level'
)
parser.add_argument(
    '-s', '--seed',
    dest='seed',
    metavar='SEED',
    type=int,
    default=0,
    help='Numpy random seed'
)
parser.add_argument(
    '-x', '--xml-ann-file',
    dest='xml_ann_file',
    metavar='XML-ANNOTATION-FILE',
    type=argparse.FileType('r'),
    help='XML annotation file'
)
parser.add_argument(
    '-r', '--results-sdir',
    dest='results_sdir',
    metavar='RESULTS-SUBDIR',
    type=str,
    help='results subdir name under `config[settings][results_dir]`. Default: auto-generated as a "MMDD-hhmm" timestamp'
)

# easier with a dict
args = vars(parser.parse_args())

# prepare results directory
results_sdir = args['results_sdir'] if args['results_sdir'] else time_stamp.strftime('%m%d-%H%M')
new_results_dir = os.path.expanduser(
    os.path.join(config['settings']['results_dir'], results_sdir)
)
try:
    os.makedirs(new_results_dir)
except OSError as e:
    if e.errno == errno.EEXIST and args['results_sdir']:
        pass
    else:
        sys.exit('[error] {}: cannot make results dir: {}\n'.format(new_results_dir, e))

# start logging
logger = log_init(
    log_level=args['log_level'],
    log_fname=os.path.join(new_results_dir, 'INFO.log')
)
shutil.copy2(src=CONFIG_FILE, dst=new_results_dir)

logger.debug(
    '[cnn][config] saving configuration file ({}) to results dir: {}'.format(CONFIG_FILE, new_results_dir)
)

# conf override, None by args default
if args['gpu_id']:
    config['settings']['GPU'] = args['gpu_id']

logger.debug('config:\n%s\n' % pp.pformat(config))
logger.debug('args:\n%s\n' % pp.pformat(args))

try:
    validate_pipeline(args['pipeline'])
except argparse.ArgumentTypeError as e:
    sys.exit('[error] invalid pipeline definition: {}\n'.format(e))


################################################################################
# init
################################################################################


################################################################################

single_ann_mode = False
single_ann_file = args['xml_ann_file'] # open file oject
if single_ann_file:
    single_ann_mode = True
    wlog('[cnn][config]', 'single annotation mode. XML file: %s' % single_ann_file.name)

wlog('[cnn]', 'Setting random seed: %s' % args['seed'])
np.random.seed(args['seed'])

################################################################################
# pipeline run
################################################################################

for step in args['pipeline']:
    wlog('[cnn]', 'running step: {}'.format(step))
    try:
        eval(step)(config, new_results_dir, logger)
    except Exception as e:
        logger.debug(traceback.format_exc())
        sys.exit('[cnn] {}: pipeline step failed: {} '.format(step, e))


sys.exit(0)
